# stable_baselines3 å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

`stable_baselines3` æ˜¯ç›®å‰æœ€æµè¡Œçš„Pythonå¼ºåŒ–å­¦ä¹ åº“ä¹‹ä¸€ï¼Œæä¾›äº†é«˜è´¨é‡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°ï¼ŒåŒ…æ‹¬PPOã€SACã€DQNã€A2Cã€TD3ç­‰ã€‚

## ğŸš€ å®‰è£…æ–¹æ³•

### åŸºç¡€å®‰è£…
```bash
pip install stable-baselines3
```

### åŒ…å«é¢å¤–åŠŸèƒ½
```bash
pip install stable-baselines3[extra]
```

### ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬
```bash
pip install git+https://github.com/DLR-RM/stable-baselines3
```

### éªŒè¯å®‰è£…
```bash
python -c "from stable_baselines3 import PPO; print('å®‰è£…æˆåŠŸï¼')"
```

## ğŸ“¦ æ­£ç¡®çš„å¯¼å…¥æ–¹æ³•

### åŸºæœ¬ç®—æ³•
```python
from stable_baselines3 import PPO, SAC, DQN, A2C, TD3
```

### ç¯å¢ƒå·¥å…·
```python
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize
```

### å›è°ƒå‡½æ•°
```python
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
```

### ç­–ç•¥ç½‘ç»œ
```python
from stable_baselines3.common.policies import ActorCriticPolicy
```

### å·¥å…·å‡½æ•°
```python
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env
```

### ç›‘æ§å’Œæ—¥å¿—
```python
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure
```

## ğŸ”§ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

### ç®€å•è®­ç»ƒ
```python
import gymnasium as gym
from stable_baselines3 import PPO

# 1. åˆ›å»ºç¯å¢ƒ
env = gym.make('CartPole-v1')

# 2. åˆ›å»ºæ¨¡å‹
model = PPO('MlpPolicy', env, verbose=1)

# 3. è®­ç»ƒæ¨¡å‹
model.learn(total_timesteps=10000)

# 4. ä¿å­˜æ¨¡å‹
model.save("ppo_cartpole")

# 5. åŠ è½½æ¨¡å‹
model = PPO.load("ppo_cartpole")

# 6. æµ‹è¯•æ¨¡å‹
obs, _ = env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs, _ = env.reset()
```

### è‡ªå®šä¹‰ç¯å¢ƒ
```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from stable_baselines3 import PPO

class CustomEnv(gym.Env):
    def __init__(self):
        super(CustomEnv, self).__init__()
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32
        )
        
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.state = self.np_random.normal(size=(4,))
        return self.state.astype(np.float32), {}
    
    def step(self, action):
        self.state += self.np_random.normal(size=(4,)) * 0.1
        reward = 1.0 if action == 1 else -1.0
        terminated = False
        truncated = False
        info = {}
        return self.state.astype(np.float32), reward, terminated, truncated, info

# ä½¿ç”¨è‡ªå®šä¹‰ç¯å¢ƒ
env = CustomEnv()
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=5000)
```

## ğŸ“ å›è°ƒå‡½æ•°ä½¿ç”¨

```python
from stable_baselines3.common.callbacks import BaseCallback

class CustomCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(CustomCallback, self).__init__(verbose)
        self.best_mean_reward = -np.inf
        
    def _on_training_start(self) -> None:
        print("è®­ç»ƒå¼€å§‹")
        
    def _on_rollout_start(self) -> None:
        print("å›æ»šå¼€å§‹")
        
    def _on_step(self) -> bool:
        # æ¯æ­¥è°ƒç”¨
        if self.n_calls % 1000 == 0:
            print(f"æ­¥æ•°: {self.n_calls}")
        return True  # ç»§ç»­è®­ç»ƒ
        
    def _on_rollout_end(self) -> None:
        print("å›æ»šç»“æŸ")
        
    def _on_training_end(self) -> None:
        print("è®­ç»ƒç»“æŸ")

# ä½¿ç”¨å›è°ƒå‡½æ•°
callback = CustomCallback()
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=10000, callback=callback)
```

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜

### PPO è¶…å‚æ•°
```python
model = PPO(
    'MlpPolicy',
    env,
    learning_rate=3e-4,      # å­¦ä¹ ç‡
    n_steps=2048,            # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
    batch_size=64,           # æ‰¹é‡å¤§å°
    n_epochs=10,             # ä¼˜åŒ–è½®æ•°
    gamma=0.99,              # æŠ˜æ‰£å› å­
    gae_lambda=0.95,         # GAEå‚æ•°
    clip_range=0.2,          # PPOè£å‰ªå‚æ•°
    ent_coef=0.01,           # ç†µç³»æ•°
    vf_coef=0.5,             # ä»·å€¼å‡½æ•°ç³»æ•°
    max_grad_norm=0.5,       # æ¢¯åº¦è£å‰ª
    verbose=1,
    device='auto'            # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
)
```

### SAC è¶…å‚æ•°
```python
from stable_baselines3 import SAC

model = SAC(
    'MlpPolicy',
    env,
    learning_rate=3e-4,
    buffer_size=1000000,     # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    learning_starts=100,     # å¼€å§‹å­¦ä¹ çš„æ­¥æ•°
    batch_size=256,          # æ‰¹é‡å¤§å°
    tau=0.005,               # è½¯æ›´æ–°å‚æ•°
    gamma=0.99,              # æŠ˜æ‰£å› å­
    train_freq=1,            # è®­ç»ƒé¢‘ç‡
    gradient_steps=1,        # æ¢¯åº¦æ›´æ–°æ­¥æ•°
    verbose=1
)
```

## ğŸ› å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### 1. å¯¼å…¥é”™è¯¯
**é”™è¯¯**: `ImportError: No module named 'stable_baselines3'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install stable-baselines3
# æˆ–è€…æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæ˜¯å¦æ¿€æ´»
```

### 2. gymnasium vs gym å†²çª
**é”™è¯¯**: `AttributeError: module 'gym' has no attribute 'make'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¸è½½æ—§ç‰ˆgymï¼Œå®‰è£…gymnasium
pip uninstall gym
pip install gymnasium

# ä»£ç ä¸­ä½¿ç”¨
import gymnasium as gym
```

### 3. ç¯å¢ƒç±»å‹é”™è¯¯
**é”™è¯¯**: `ValueError: The environment must inherit from gymnasium.Env`

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿ç»§æ‰¿æ­£ç¡®çš„åŸºç±»
import gymnasium as gym

class MyEnv(gym.Env):  # ç»§æ‰¿gymnasium.Env
    def __init__(self):
        super(MyEnv, self).__init__()
        # ...
```

### 4. åŠ¨ä½œç©ºé—´é”™è¯¯
**é”™è¯¯**: `AssertionError: The action space must be of type gymnasium.spaces`

**è§£å†³æ–¹æ¡ˆ**:
```python
from gymnasium import spaces

# ä½¿ç”¨gymnasium.spaceså®šä¹‰ç©ºé—´
self.action_space = spaces.Discrete(2)
self.observation_space = spaces.Box(low=-1, high=1, shape=(4,))
```

### 5. CUDAå†…å­˜ä¸è¶³
**é”™è¯¯**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨CPUè®­ç»ƒ
model = PPO('MlpPolicy', env, device='cpu')

# æˆ–å‡å°‘æ‰¹é‡å¤§å°
model = PPO('MlpPolicy', env, batch_size=32)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç¯å¢ƒéªŒè¯
```python
from stable_baselines3.common.env_checker import check_env

# éªŒè¯è‡ªå®šä¹‰ç¯å¢ƒ
check_env(env, warn=True)
```

### 2. ç¯å¢ƒæ ‡å‡†åŒ–
```python
from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv

# åŒ…è£…ç¯å¢ƒ
env = DummyVecEnv([lambda: env])
env = VecNormalize(env, norm_obs=True, norm_reward=True)
```

### 3. å¹¶è¡Œè®­ç»ƒ
```python
from stable_baselines3.common.vec_env import SubprocVecEnv

# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for _ in range(4)])
```

### 4. æ¨¡å‹è¯„ä¼°
```python
from stable_baselines3.common.evaluation import evaluate_policy

# è¯„ä¼°æ¨¡å‹æ€§èƒ½
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
print(f"å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
```

### 5. TensorBoardç›‘æ§
```python
# å¯ç”¨TensorBoardæ—¥å¿—
model = PPO('MlpPolicy', env, tensorboard_log="./ppo_tensorboard/")

# æŸ¥çœ‹æ—¥å¿—
# tensorboard --logdir ./ppo_tensorboard/
```

### 6. æ£€æŸ¥ç‚¹ä¿å­˜
```python
from stable_baselines3.common.callbacks import CheckpointCallback

# å®šæœŸä¿å­˜æ¨¡å‹
checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')
model.learn(total_timesteps=10000, callback=checkpoint_callback)
```

## ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

```python
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecNormalize
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.evaluation import evaluate_policy

# 1. åˆ›å»ºç¯å¢ƒ
env = make_vec_env('CartPole-v1', n_envs=4)
env = VecNormalize(env, norm_obs=True, norm_reward=True)

# 2. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
eval_env = make_vec_env('CartPole-v1', n_envs=1)
eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, training=False)

# 3. åˆ›å»ºå›è°ƒå‡½æ•°
eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',
                            log_path='./logs/', eval_freq=500,
                            deterministic=True, render=False)

checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')

# 4. åˆ›å»ºæ¨¡å‹
model = PPO('MlpPolicy', env, verbose=1, tensorboard_log="./ppo_tensorboard/")

# 5. è®­ç»ƒæ¨¡å‹
model.learn(total_timesteps=50000, callback=[eval_callback, checkpoint_callback])

# 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
model.save("ppo_final")

# 7. è¯„ä¼°æ¨¡å‹
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)
print(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: {mean_reward:.2f} Â± {std_reward:.2f}")
```

## ğŸ“š ç®—æ³•é€‰æ‹©æŒ‡å—

### PPO (Proximal Policy Optimization)
- **é€‚ç”¨äº**: è¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
- **ç‰¹ç‚¹**: ç¨³å®šã€æ˜“è°ƒå‚
- **æ¨èåœºæ™¯**: é€šç”¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡

### SAC (Soft Actor-Critic)
- **é€‚ç”¨äº**: è¿ç»­åŠ¨ä½œç©ºé—´
- **ç‰¹ç‚¹**: æ ·æœ¬æ•ˆç‡é«˜ã€æ¢ç´¢æ€§å¥½
- **æ¨èåœºæ™¯**: æœºå™¨äººæ§åˆ¶ã€è¿ç»­æ§åˆ¶ä»»åŠ¡

### DQN (Deep Q-Network)
- **é€‚ç”¨äº**: ç¦»æ•£åŠ¨ä½œç©ºé—´
- **ç‰¹ç‚¹**: ç»å…¸ç®—æ³•ã€æ˜“ç†è§£
- **æ¨èåœºæ™¯**: æ¸¸æˆã€ç¦»æ•£å†³ç­–ä»»åŠ¡

### A2C (Advantage Actor-Critic)
- **é€‚ç”¨äº**: è¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
- **ç‰¹ç‚¹**: ç®€å•ã€å¿«é€Ÿ
- **æ¨èåœºæ™¯**: ç®€å•ä»»åŠ¡ã€åŸå‹å¼€å‘

### TD3 (Twin Delayed DDPG)
- **é€‚ç”¨äº**: è¿ç»­åŠ¨ä½œç©ºé—´
- **ç‰¹ç‚¹**: ç¨³å®šçš„ç¡®å®šæ€§ç­–ç•¥
- **æ¨èåœºæ™¯**: è¿ç»­æ§åˆ¶ã€éœ€è¦ç¡®å®šæ€§ç­–ç•¥çš„ä»»åŠ¡

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://stable-baselines3.readthedocs.io/
- **GitHubä»“åº“**: https://github.com/DLR-RM/stable-baselines3
- **ç¤ºä¾‹ä»£ç **: https://github.com/DLR-RM/rl-baselines3-zoo
- **å­¦æœ¯è®ºæ–‡**: https://jmlr.org/papers/v22/20-1364.html
- **æ•™ç¨‹è§†é¢‘**: https://www.youtube.com/c/MachineLearningwithPhil

## âœ… æ€»ç»“

stable_baselines3 æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”æ˜“ç”¨çš„å¼ºåŒ–å­¦ä¹ åº“ã€‚æ­£ç¡®ä½¿ç”¨å®ƒçš„å…³é”®è¦ç‚¹ï¼š

1. **å®‰è£…**: ä½¿ç”¨ `pip install stable-baselines3`
2. **å¯¼å…¥**: ä½¿ç”¨å®Œæ•´çš„æ¨¡å—è·¯å¾„å¯¼å…¥
3. **ç¯å¢ƒ**: ä½¿ç”¨ gymnasium è€Œä¸æ˜¯æ—§ç‰ˆ gym
4. **éªŒè¯**: ä½¿ç”¨ check_env éªŒè¯è‡ªå®šä¹‰ç¯å¢ƒ
5. **ç›‘æ§**: ä½¿ç”¨å›è°ƒå‡½æ•°å’ŒTensorBoardç›‘æ§è®­ç»ƒ
6. **è¯„ä¼°**: ä½¿ç”¨ evaluate_policy è¯„ä¼°æ¨¡å‹æ€§èƒ½

éµå¾ªè¿™äº›æœ€ä½³å®è·µï¼Œæ‚¨å°±èƒ½å¤ŸæˆåŠŸä½¿ç”¨ stable_baselines3 è¿›è¡Œå¼ºåŒ–å­¦ä¹ é¡¹ç›®å¼€å‘ã€‚