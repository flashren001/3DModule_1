#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„å¼ºåŒ–å­¦ä¹ æ¼”ç¤º - å±•ç¤ºstable_baselines3çš„æ­£ç¡®ä½¿ç”¨æ–¹æ³•
ä¸ä¾èµ–å¤–éƒ¨æ·±åº¦å­¦ä¹ åº“ï¼Œçº¯Pythonå®ç°
"""

import random
import math
from typing import Dict, List, Tuple, Any
import json


# æ¨¡æ‹Ÿ stable_baselines3 çš„ä½¿ç”¨æ–¹å¼
class StableBaselines3Demo:
    """å±•ç¤º stable_baselines3 çš„æ­£ç¡®å¼•ç”¨å’Œä½¿ç”¨æ–¹æ³•"""
    
    def __init__(self):
        print("=== Stable Baselines3 ä½¿ç”¨æ–¹æ³•æ¼”ç¤º ===\n")
    
    def show_correct_imports(self):
        """å±•ç¤ºæ­£ç¡®çš„å¯¼å…¥æ–¹æ³•"""
        print("ğŸ“¦ æ­£ç¡®çš„ stable_baselines3 å¯¼å…¥æ–¹æ³•:")
        print("""
# 1. åŸºæœ¬ç®—æ³•å¯¼å…¥
from stable_baselines3 import PPO, SAC, DQN, A2C, TD3

# 2. ç¯å¢ƒå·¥å…·å¯¼å…¥
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv

# 3. å›è°ƒå‡½æ•°å¯¼å…¥
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback

# 4. ç­–ç•¥ç½‘ç»œå¯¼å…¥
from stable_baselines3.common.policies import ActorCriticPolicy

# 5. å·¥å…·å‡½æ•°å¯¼å…¥
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.evaluation import evaluate_policy

# 6. ç›‘æ§å’Œæ—¥å¿—
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure
        """)
        print("-" * 60)
    
    def show_installation_methods(self):
        """å±•ç¤ºå®‰è£…æ–¹æ³•"""
        print("ğŸ“‹ stable_baselines3 å®‰è£…æ–¹æ³•:")
        print("""
# æ–¹æ³•1: åŸºç¡€å®‰è£…
pip install stable-baselines3

# æ–¹æ³•2: åŒ…å«é¢å¤–åŠŸèƒ½
pip install stable-baselines3[extra]

# æ–¹æ³•3: ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install git+https://github.com/DLR-RM/stable-baselines3

# æ–¹æ³•4: ç‰¹å®šç‰ˆæœ¬
pip install stable-baselines3==2.0.0

# æ£€æŸ¥å®‰è£…æ˜¯å¦æˆåŠŸ
python -c "from stable_baselines3 import PPO; print('å®‰è£…æˆåŠŸ')"
        """)
        print("-" * 60)
    
    def show_basic_usage(self):
        """å±•ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
        print("ğŸš€ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹:")
        print("""
import gymnasium as gym
from stable_baselines3 import PPO

# 1. åˆ›å»ºç¯å¢ƒ
env = gym.make('CartPole-v1')

# 2. åˆ›å»ºPPOæ¨¡å‹
model = PPO('MlpPolicy', env, verbose=1)

# 3. è®­ç»ƒæ¨¡å‹
model.learn(total_timesteps=10000)

# 4. ä¿å­˜æ¨¡å‹
model.save("ppo_cartpole")

# 5. åŠ è½½æ¨¡å‹
model = PPO.load("ppo_cartpole")

# 6. æµ‹è¯•æ¨¡å‹
obs, _ = env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs, _ = env.reset()
        """)
        print("-" * 60)
    
    def show_custom_environment(self):
        """å±•ç¤ºè‡ªå®šä¹‰ç¯å¢ƒçš„ä½¿ç”¨"""
        print("ğŸ—ï¸ è‡ªå®šä¹‰ç¯å¢ƒç¤ºä¾‹:")
        print("""
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from stable_baselines3 import PPO

class CustomEnv(gym.Env):
    def __init__(self):
        super(CustomEnv, self).__init__()
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32
        )
        
    def reset(self, seed=None, options=None):
        self.state = np.random.randn(4)
        return self.state, {}
    
    def step(self, action):
        # å®ç°ç¯å¢ƒé€»è¾‘
        self.state += np.random.randn(4) * 0.1
        reward = 1.0 if action == 1 else -1.0
        terminated = False
        truncated = False
        info = {}
        return self.state, reward, terminated, truncated, info

# ä½¿ç”¨è‡ªå®šä¹‰ç¯å¢ƒ
env = CustomEnv()
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=5000)
        """)
        print("-" * 60)
    
    def show_callback_usage(self):
        """å±•ç¤ºå›è°ƒå‡½æ•°çš„ä½¿ç”¨"""
        print("ğŸ“ å›è°ƒå‡½æ•°ä½¿ç”¨ç¤ºä¾‹:")
        print("""
from stable_baselines3.common.callbacks import BaseCallback

class CustomCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(CustomCallback, self).__init__(verbose)
        
    def _on_training_start(self) -> None:
        print("è®­ç»ƒå¼€å§‹")
        
    def _on_rollout_start(self) -> None:
        print("å›æ»šå¼€å§‹")
        
    def _on_step(self) -> bool:
        # æ¯æ­¥è°ƒç”¨
        if self.n_calls % 1000 == 0:
            print(f"æ­¥æ•°: {self.n_calls}")
        return True  # ç»§ç»­è®­ç»ƒ
        
    def _on_rollout_end(self) -> None:
        print("å›æ»šç»“æŸ")
        
    def _on_training_end(self) -> None:
        print("è®­ç»ƒç»“æŸ")

# ä½¿ç”¨å›è°ƒå‡½æ•°
callback = CustomCallback()
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=10000, callback=callback)
        """)
        print("-" * 60)
    
    def show_hyperparameter_tuning(self):
        """å±•ç¤ºè¶…å‚æ•°è°ƒä¼˜"""
        print("âš™ï¸ è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹:")
        print("""
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

# PPO è¶…å‚æ•°ç¤ºä¾‹
model = PPO(
    'MlpPolicy',
    env,
    learning_rate=3e-4,      # å­¦ä¹ ç‡
    n_steps=2048,            # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
    batch_size=64,           # æ‰¹é‡å¤§å°
    n_epochs=10,             # ä¼˜åŒ–è½®æ•°
    gamma=0.99,              # æŠ˜æ‰£å› å­
    gae_lambda=0.95,         # GAEå‚æ•°
    clip_range=0.2,          # PPOè£å‰ªå‚æ•°
    ent_coef=0.01,           # ç†µç³»æ•°
    vf_coef=0.5,             # ä»·å€¼å‡½æ•°ç³»æ•°
    max_grad_norm=0.5,       # æ¢¯åº¦è£å‰ª
    verbose=1
)

# SAC è¶…å‚æ•°ç¤ºä¾‹  
from stable_baselines3 import SAC

model = SAC(
    'MlpPolicy',
    env,
    learning_rate=3e-4,
    buffer_size=1000000,     # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    learning_starts=100,     # å¼€å§‹å­¦ä¹ çš„æ­¥æ•°
    batch_size=256,          # æ‰¹é‡å¤§å°
    tau=0.005,               # è½¯æ›´æ–°å‚æ•°
    gamma=0.99,              # æŠ˜æ‰£å› å­
    train_freq=1,            # è®­ç»ƒé¢‘ç‡
    gradient_steps=1,        # æ¢¯åº¦æ›´æ–°æ­¥æ•°
    verbose=1
)
        """)
        print("-" * 60)
    
    def show_common_errors_and_solutions(self):
        """å±•ç¤ºå¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ"""
        print("ğŸ› å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ:")
        
        errors = [
            {
                "error": "ImportError: No module named 'stable_baselines3'",
                "solution": "pip install stable-baselines3"
            },
            {
                "error": "AttributeError: module 'gym' has no attribute 'make'",
                "solution": "ä½¿ç”¨ gymnasium æ›¿ä»£ gym:\npip install gymnasium"
            },
            {
                "error": "ValueError: The environment must inherit from gymnasium.Env",
                "solution": "ç¡®ä¿è‡ªå®šä¹‰ç¯å¢ƒç»§æ‰¿è‡ª gymnasium.Env"
            },
            {
                "error": "AssertionError: The action space must be of type gymnasium.spaces",
                "solution": "ä½¿ç”¨ gymnasium.spaces å®šä¹‰åŠ¨ä½œç©ºé—´"
            },
            {
                "error": "CUDA out of memory",
                "solution": "ä½¿ç”¨CPUè®­ç»ƒæˆ–å‡å°‘æ‰¹é‡å¤§å°:\nmodel = PPO('MlpPolicy', env, device='cpu')"
            }
        ]
        
        for i, item in enumerate(errors, 1):
            print(f"{i}. é”™è¯¯: {item['error']}")
            print(f"   è§£å†³: {item['solution']}\n")
        
        print("-" * 60)
    
    def simulate_rl_training(self):
        """æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹"""
        print("ğŸ¯ æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹:")
        
        # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
        total_timesteps = 10000
        n_steps = 2048
        
        print(f"å¼€å§‹è®­ç»ƒ PPO æ¨¡å‹...")
        print(f"æ€»æ—¶é—´æ­¥: {total_timesteps}")
        print(f"æ¯æ¬¡æ›´æ–°æ­¥æ•°: {n_steps}")
        print()
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for update in range(1, total_timesteps // n_steps + 1):
            # æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡
            mean_reward = random.uniform(50, 200) + update * 5
            std_reward = random.uniform(10, 30)
            success_rate = min(0.95, 0.1 + update * 0.1)
            
            print(f"æ›´æ–° {update}:")
            print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
            print(f"  æˆåŠŸç‡: {success_rate:.2%}")
            print(f"  è®­ç»ƒæ­¥æ•°: {update * n_steps}")
            print()
            
            if update >= 3:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªæ›´æ–°
                break
        
        print("è®­ç»ƒå®Œæˆ!")
        print("-" * 60)
    
    def show_best_practices(self):
        """å±•ç¤ºæœ€ä½³å®è·µ"""
        print("ğŸ’¡ stable_baselines3 æœ€ä½³å®è·µ:")
        
        practices = [
            "1. ç¯å¢ƒæ ‡å‡†åŒ–: ä½¿ç”¨ VecNormalize åŒ…è£…ç¯å¢ƒ",
            "2. è¶…å‚æ•°è°ƒä¼˜: ä½¿ç”¨ Optuna æˆ–ç½‘æ ¼æœç´¢",
            "3. æ¨¡å‹è¯„ä¼°: ä½¿ç”¨ evaluate_policy å‡½æ•°",
            "4. è®­ç»ƒç›‘æ§: ä½¿ç”¨ TensorBoard è®°å½•æ—¥å¿—",
            "5. æ¨¡å‹ä¿å­˜: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹",
            "6. ç¯å¢ƒéªŒè¯: ä½¿ç”¨ check_env éªŒè¯è‡ªå®šä¹‰ç¯å¢ƒ",
            "7. å¹¶è¡Œè®­ç»ƒ: ä½¿ç”¨ SubprocVecEnv åŠ é€Ÿè®­ç»ƒ",
            "8. ç»éªŒå›æ”¾: å¯¹äºç¦»çº¿ç®—æ³•ï¼Œç¡®ä¿è¶³å¤Ÿçš„ç¼“å†²åŒºå¤§å°"
        ]
        
        for practice in practices:
            print(f"  {practice}")
        
        print("\nç¤ºä¾‹ä»£ç :")
        print("""
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv
from stable_baselines3.common.evaluation import evaluate_policy

# 1. æ£€æŸ¥ç¯å¢ƒ
check_env(env)

# 2. å¹¶è¡Œç¯å¢ƒ
env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for _ in range(4)])

# 3. ç¯å¢ƒæ ‡å‡†åŒ–
env = VecNormalize(env, norm_obs=True, norm_reward=True)

# 4. æ¨¡å‹è®­ç»ƒ
model = PPO('MlpPolicy', env, verbose=1, tensorboard_log="./ppo_tensorboard/")
model.learn(total_timesteps=100000)

# 5. æ¨¡å‹è¯„ä¼°
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
print(f"å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
        """)
        print("-" * 60)
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ‰ Stable Baselines3 å®Œæ•´ä½¿ç”¨æŒ‡å—")
        print("=" * 60)
        
        self.show_installation_methods()
        self.show_correct_imports()
        self.show_basic_usage()
        self.show_custom_environment()
        self.show_callback_usage()
        self.show_hyperparameter_tuning()
        self.show_common_errors_and_solutions()
        self.simulate_rl_training()
        self.show_best_practices()
        
        print("ğŸ“š æ›´å¤šèµ„æº:")
        print("  - å®˜æ–¹æ–‡æ¡£: https://stable-baselines3.readthedocs.io/")
        print("  - GitHub: https://github.com/DLR-RM/stable-baselines3")
        print("  - ç¤ºä¾‹ä»£ç : https://github.com/DLR-RM/rl-baselines3-zoo")
        print("  - è®ºæ–‡: https://jmlr.org/papers/v22/20-1364.html")
        print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo = StableBaselines3Demo()
    demo.run_complete_demo()